{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "from sys import stderr\n",
    "import dask_cudf as dcd\n",
    "import pandas as pd\n",
    "from utils import data_cols, date_cols, reduction_cds, card_types, payment_types, dtypes\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(stream=stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading November Views for Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiobotocore.credentials:Found credentials from IAM Role: S3_Access\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.9 s, sys: 8.76 s, total: 1min 3s\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "order_details = pd.read_parquet(\n",
    "        \"s3://otg-prod-fraud-data/prod_data/updated_unhashed_joins/order_details/may-june.parquet\",\n",
    "        # dtype=dtype,\n",
    "        # parse_dates=[\"bus_date\", \"birth_date\"],\n",
    "    ).sort_values(by=\"bus_date\").reset_index(drop=True)\n",
    "\n",
    "order_details = order_details.drop(\n",
    "    order_details.columns.difference(data_cols), axis=\"columns\"\n",
    ")\n",
    "order_details[\"reduction_cd\"] = order_details[\"reduction_cd\"].replace(\n",
    "    to_replace=reduction_cds\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5516291, 13)\n",
      "2024-05-01 00:00:00 2024-06-30 00:00:00\n",
      "CPU times: user 28.9 s, sys: 2.03 s, total: 30.9 s\n",
      "Wall time: 30.7 s\n",
      "CPU times: user 28.9 s, sys: 2.03 s, total: 30.9 s\n",
      "Wall time: 30.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "%%time\n",
    "\n",
    "payment_details = pd.read_parquet(\n",
    "        \"s3://otg-prod-fraud-data/prod_data/updated_unhashed_joins/payment_details/may-june.parquet\",\n",
    "        # dtype=dtype,\n",
    "        # parse_dates=[\"bus_date\", \"birth_date\"],\n",
    "    ).sort_values(by=\"bus_date\").reset_index(drop=True)\n",
    "\n",
    "payment_details = payment_details.drop(\n",
    "    payment_details.columns.difference(data_cols), axis=\"columns\"\n",
    ")\n",
    "\n",
    "payment_details[\"card_type\"] = payment_details[\"card_type\"].replace(\n",
    "    to_replace=card_types\n",
    ")\n",
    "payment_details[\"payment_type\"] = payment_details[\"payment_type\"].replace(\n",
    "    to_replace=payment_types\n",
    ")\n",
    "print(payment_details.shape)\n",
    "print(payment_details.bus_date.min(), payment_details.bus_date.max())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5443872\n",
      "5443872\n",
      "(5566992, 37)\n",
      "(5516291, 13)\n",
      "2024-05-01 00:00:00 2024-06-30 00:00:00\n",
      "2024-05-01 00:00:00 2024-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(order_details.order_id.nunique())\n",
    "print(payment_details.order_id.nunique())\n",
    "print(order_details.shape)\n",
    "print(payment_details.shape)\n",
    "print(order_details.bus_date.min(), order_details.bus_date.max())\n",
    "print(payment_details.bus_date.min(), payment_details.bus_date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bus_date', 'pos_venue_id', 'order_id', 'vendor_id'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_details.columns.intersection(payment_details.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 1.56 s, total: 24.5 s\n",
      "Wall time: 24.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = order_details.drop([\"pos_venue_id\", \"vendor_id\"], axis=1).merge(\n",
    "    payment_details.drop([\"bus_date\"], axis=1), on=[\"order_id\"], how=\"left\"\n",
    ")\n",
    "\n",
    "del order_details, payment_details\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voucher Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.32 s, sys: 0 ns, total: 3.32 s\n",
      "Wall time: 3.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#MultipleVoucherMarker\n",
    "dataset[\"voucher_count\"] = dataset.groupby('voucher_number')['device_order_id'].transform(\"nunique\").fillna(0).astype(int)\n",
    "dataset[\"FE_multiple_voucher_fl\"] = (dataset[\"voucher_count\"] > 1).astype(int)\n",
    "\n",
    "min_bus_date_mask = (\n",
    "    dataset.groupby('voucher_number')['bus_date'].transform(\"min\")\n",
    "    == dataset['bus_date']\n",
    "    )\n",
    "\n",
    "dataset[\"FE_multiple_voucher_fl\"] &= ~min_bus_date_mask.astype(int)\n",
    "\n",
    "del min_bus_date_mask\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.4 s, sys: 167 ms, total: 51.6 s\n",
      "Wall time: 51.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Unique Counts\n",
    "main_columns=[\"voucher_number\", \"order_id\"]\n",
    "uids=[\"device_order_id\", \"cust_prof_id\"]\n",
    "aggregations=[\"nunique\"]\n",
    "\n",
    "for main_column in main_columns:\n",
    "    for col in uids:\n",
    "        for agg in aggregations:\n",
    "            new_col = \"FE_\" + col + \"_\" + main_column + \"_ct_AG\"\n",
    "            uniques_df = dataset.groupby(col)[main_column].agg([agg])[agg]\n",
    "            dataset[new_col] = dataset[col].map(uniques_df).astype('Float64')\n",
    "            \n",
    "del main_column, uids, aggregations, uniques_df, new_col\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5642266, 52)\n",
      "2024-05-01 00:00:00 2024-06-30 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bus_date</th>\n",
       "      <th>order_id</th>\n",
       "      <th>device_order_id</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>order_local_time</th>\n",
       "      <th>sales_hr</th>\n",
       "      <th>ord_beg_time</th>\n",
       "      <th>ord_close_time</th>\n",
       "      <th>item_count</th>\n",
       "      <th>gross_total</th>\n",
       "      <th>...</th>\n",
       "      <th>account_id</th>\n",
       "      <th>payment_amt_rewards_points</th>\n",
       "      <th>voucher_number</th>\n",
       "      <th>emp_email</th>\n",
       "      <th>voucher_count</th>\n",
       "      <th>FE_multiple_voucher_fl</th>\n",
       "      <th>FE_device_order_id_voucher_number_ct_AG</th>\n",
       "      <th>FE_cust_prof_id_voucher_number_ct_AG</th>\n",
       "      <th>FE_device_order_id_order_id_ct_AG</th>\n",
       "      <th>FE_cust_prof_id_order_id_ct_AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>14864239601</td>\n",
       "      <td>240425017000824</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2024-04-25 16:30:16</td>\n",
       "      <td>16</td>\n",
       "      <td>2024-04-25 20:30:16</td>\n",
       "      <td>2024-04-25 20:30:16</td>\n",
       "      <td>3</td>\n",
       "      <td>-32.25</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>149058443</td>\n",
       "      <td>A0000916917145627971504</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2024-05-01 07:38:39</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-05-01 11:38:39</td>\n",
       "      <td>2024-05-01 11:38:39</td>\n",
       "      <td>1</td>\n",
       "      <td>4.55</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>149058438</td>\n",
       "      <td>240501009000119</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2024-05-01 07:38:37</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-05-01 11:38:37</td>\n",
       "      <td>2024-05-01 11:38:37</td>\n",
       "      <td>5</td>\n",
       "      <td>42.75</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>149058431</td>\n",
       "      <td>240501026000109</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2024-05-01 07:38:33</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-05-01 11:38:33</td>\n",
       "      <td>2024-05-01 11:38:33</td>\n",
       "      <td>4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>149058431</td>\n",
       "      <td>240501026000109</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2024-05-01 07:38:33</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-05-01 11:38:33</td>\n",
       "      <td>2024-05-01 11:38:33</td>\n",
       "      <td>6</td>\n",
       "      <td>21.19</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    bus_date     order_id          device_order_id         time_zone  \\\n",
       "0 2024-05-01  14864239601          240425017000824  America/New_York   \n",
       "1 2024-05-01    149058443  A0000916917145627971504  America/New_York   \n",
       "2 2024-05-01    149058438          240501009000119  America/New_York   \n",
       "3 2024-05-01    149058431          240501026000109  America/New_York   \n",
       "4 2024-05-01    149058431          240501026000109  America/New_York   \n",
       "\n",
       "     order_local_time  sales_hr        ord_beg_time      ord_close_time  \\\n",
       "0 2024-04-25 16:30:16        16 2024-04-25 20:30:16 2024-04-25 20:30:16   \n",
       "1 2024-05-01 07:38:39         7 2024-05-01 11:38:39 2024-05-01 11:38:39   \n",
       "2 2024-05-01 07:38:37         7 2024-05-01 11:38:37 2024-05-01 11:38:37   \n",
       "3 2024-05-01 07:38:33         7 2024-05-01 11:38:33 2024-05-01 11:38:33   \n",
       "4 2024-05-01 07:38:33         7 2024-05-01 11:38:33 2024-05-01 11:38:33   \n",
       "\n",
       "   item_count  gross_total  ...  account_id  payment_amt_rewards_points  \\\n",
       "0           3       -32.25  ...        <NA>                        <NA>   \n",
       "1           1         4.55  ...        <NA>                        <NA>   \n",
       "2           5        42.75  ...        <NA>                        <NA>   \n",
       "3           4          7.6  ...        <NA>                        <NA>   \n",
       "4           6        21.19  ...        <NA>                        <NA>   \n",
       "\n",
       "   voucher_number emp_email  voucher_count FE_multiple_voucher_fl  \\\n",
       "0            <NA>      <NA>              0                      0   \n",
       "1            <NA>      <NA>              0                      0   \n",
       "2            <NA>      <NA>              0                      0   \n",
       "3            <NA>      <NA>              0                      0   \n",
       "4            <NA>      <NA>              0                      0   \n",
       "\n",
       "  FE_device_order_id_voucher_number_ct_AG  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "\n",
       "   FE_cust_prof_id_voucher_number_ct_AG  FE_device_order_id_order_id_ct_AG  \\\n",
       "0                                  <NA>                                1.0   \n",
       "1                                  <NA>                                1.0   \n",
       "2                                   0.0                                1.0   \n",
       "3                                   0.0                                1.0   \n",
       "4                                   0.0                                1.0   \n",
       "\n",
       "  FE_cust_prof_id_order_id_ct_AG  \n",
       "0                           <NA>  \n",
       "1                           <NA>  \n",
       "2                            2.0  \n",
       "3                            1.0  \n",
       "4                            1.0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.bus_date.min(), dataset.bus_date.max())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def non_nan_unique(row):\n",
    "    non_nan_values = [value for value in row if pd.notna(value) and value != '']\n",
    "    unique_non_nan_values = set(non_nan_values)\n",
    "    \n",
    "    if len(unique_non_nan_values) == 0:\n",
    "        return np.nan\n",
    "    elif len(unique_non_nan_values) == 1:\n",
    "        return unique_non_nan_values.pop()\n",
    "    elif len(unique_non_nan_values) == 2 and unique_non_nan_values == {0, 1}:\n",
    "        return 1\n",
    "    else:\n",
    "        raise ValueError(\"Logic Error: More than one unique non-NaN value\")\n",
    "    \n",
    "def sum_elements(lst):\n",
    "    filtered_lst = [abs(x) for x in lst if not pd.isna(x)]\n",
    "    return float(sum(filtered_lst)) if filtered_lst else np.nan\n",
    "\n",
    "def max_element(lst):\n",
    "    cleaned_list = [abs(float(x)) for x in lst if not pd.isna(x)]\n",
    "    return float(max(cleaned_list)) if cleaned_list else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Joining Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 610 ms, total: 18.4 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aggregation_dict = {fea: list for fea in dataset.columns}\n",
    "moids = dataset.order_id.value_counts()[dataset.order_id.value_counts()>1].index\n",
    "dups_df = dataset.loc[dataset.order_id.isin(moids),:]\n",
    "nondups_df = dataset.loc[~dataset.order_id.isin(moids),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 4s, sys: 684 ms, total: 4min 5s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aop = dups_df.groupby('order_id').agg(aggregation_dict)\n",
    "aop = aop.drop(columns='order_id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 455 ms, total: 16 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sum_cols = ['gross_total', 'net_total', 'taxes', 'reduction_amt', 'fee_amt', 'tip_amount', 'item_count', 'payment_amount', 'voucher_count']\n",
    "watch_cols = ['order_id', 'card_type', 'payment_type', 'transaction_seq_nu', 'voucher_number', 'menu_vendor_id']\n",
    "\n",
    "for col in sum_cols:\n",
    "    # print(col)\n",
    "    aop[col] = aop[col].apply(sum_elements)\n",
    "\n",
    "for col in aop.columns:\n",
    "    if col not in sum_cols + watch_cols:\n",
    "        aop[col] = aop[col].apply(non_nan_unique)\n",
    "\n",
    "aop['transaction_seq_nu'] = aop['transaction_seq_nu'].apply(max_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_cols = ['order_id', 'card_type', 'payment_type', 'transaction_seq_nu', 'voucher_number', 'menu_vendor_id']\n",
    "watch_cols.remove('transaction_seq_nu')\n",
    "watch_cols.remove('order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62646/1299116880.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nondups_df.loc[:,col] = nondups_df[col].apply(lambda x:[x])\n",
      "/tmp/ipykernel_62646/1299116880.py:2: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  nondups_df.loc[:,col] = nondups_df[col].apply(lambda x:[x])\n"
     ]
    }
   ],
   "source": [
    "for col in watch_cols:\n",
    "    nondups_df.loc[:,col] = nondups_df[col].apply(lambda x:[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170299, 52) (368693, 52) (5273573, 52) (5443872, 52)\n",
      "CPU times: user 10.6 s, sys: 2.87 s, total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = (\n",
    "    pd.concat([nondups_df, aop], axis=0)\n",
    "    .sort_values(by=\"bus_date\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(aop.shape, dups_df.shape, nondups_df.shape, dataset.shape)\n",
    "del aop, moids, dups_df, nondups_df, aggregation_dict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [CARD]\n",
       "1                [CARD]\n",
       "2                [CARD]\n",
       "3                [CARD]\n",
       "4                [CARD]\n",
       "               ...     \n",
       "5443867     [APPLE_PAY]\n",
       "5443868          [CARD]\n",
       "5443869          [CARD]\n",
       "5443870          [CARD]\n",
       "5443871    [CARD, CARD]\n",
       "Name: payment_type, Length: 5443872, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['payment_type'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_payment_types(payment_list):\n",
    "    counts = {}\n",
    "    if payment_list is None:\n",
    "        return counts\n",
    "    for payment in payment_list:\n",
    "        if payment is not None:\n",
    "            counts[payment] = counts.get(payment, 0) + 1\n",
    "        if payment is None:\n",
    "            counts[\"None\"] = counts.get(\"None\", 0) + 1\n",
    "    return counts\n",
    "\n",
    "dataset[\"payment_type\"] = dataset[\"payment_type\"].apply(lambda x: [\"None\" if item is pd.NA else item for item in x] if x else None)\n",
    "dataset[\"card_type\"] = dataset[\"card_type\"].apply(lambda x: [\"None\" if item is pd.NA else item for item in x] if x else None)\n",
    "\n",
    "payment_counts = dataset[\"payment_type\"].apply(count_payment_types)\n",
    "card_counts = dataset[\"card_type\"].apply(count_payment_types)\n",
    "\n",
    "payment_counts_df = pd.DataFrame(list(payment_counts)).fillna(0).astype(int)\n",
    "card_counts_df = pd.DataFrame(list(card_counts)).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_counts_df = payment_counts_df.rename(lambda x: 'payment_' + x.lower(), axis=1)\n",
    "card_counts_df = card_counts_df.rename(lambda x: 'card_' + x.lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.02 s, sys: 650 ms, total: 5.67 s\n",
      "Wall time: 5.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "862"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = pd.concat([dataset, payment_counts_df, card_counts_df], axis=1)\n",
    "\n",
    "del payment_counts, card_counts, payment_counts_df, card_counts_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bus_date', 'order_id', 'device_order_id', 'time_zone',\n",
       "       'order_local_time', 'sales_hr', 'ord_beg_time', 'ord_close_time',\n",
       "       'item_count', 'gross_total', 'net_total', 'taxes', 'reduction_amt',\n",
       "       'cust_prof_id', 'mobile_ord_fl', 'original_order_id', 'vendor_loc_id',\n",
       "       'fee_amt', 'tip_amount', 'sync_status', 'menu_vendor_id',\n",
       "       'refund_vendor_id', 'refund_venue_id', 'pos_terminal_id',\n",
       "       'reduction_cd', 'order_tab_id', 'order_status_cd', 'united_account_id',\n",
       "       'cust_first_name', 'cust_last_name', 'activity_status_code',\n",
       "       'employee_role_name', 'employer_id', 'employer_name',\n",
       "       'default_email_address', 'pos_venue_id', 'card_type', 'payment_type',\n",
       "       'vendor_id', 'transaction_seq_nu', 'cash_recycler_tiny_code',\n",
       "       'payment_amount', 'account_id', 'payment_amt_rewards_points',\n",
       "       'voucher_number', 'emp_email', 'voucher_count',\n",
       "       'FE_multiple_voucher_fl', 'FE_device_order_id_voucher_number_ct_AG',\n",
       "       'FE_cust_prof_id_voucher_number_ct_AG',\n",
       "       'FE_device_order_id_order_id_ct_AG', 'FE_cust_prof_id_order_id_ct_AG',\n",
       "       'payment_card', 'payment_cash', 'payment_apple_pay',\n",
       "       'payment_united_miles', 'payment_manual_cash', 'payment_paypal',\n",
       "       'payment_none', 'payment_chase reimbursements', 'payment_voucher',\n",
       "       'card_mastercard', 'card_american express', 'card_none', 'card_visa',\n",
       "       'card_credit', 'card_discover', 'card_jcb', 'card_voucher', 'card_cup',\n",
       "       'card_up', 'card_checkout', 'card_visa credit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2728742\n",
       "5    2715130\n",
       "Name: bus_date, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.bus_date.dt.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.bus_date.dt.month == 5].to_parquet(\n",
    "    \"s3://otg-prod-fraud-data/test_data/new/may24joined.parquet\", index=False\n",
    ")\n",
    "dataset[dataset.bus_date.dt.month == 6].to_parquet(\n",
    "    \"s3://otg-prod-fraud-data/test_data/new/june24joined.parquet\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del agg, col, main_columns, sum_cols, watch_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5443872, 73)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 627 ms, total: 2.71 s\n",
      "Wall time: 3.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4873407, 72)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dataset = dcd.concat(\n",
    "    [\n",
    "        dcd.read_parquet(\n",
    "            \"s3://otg-prod-fraud-data/test_data/oct24joined.parquet\",\n",
    "            sep=\",\",\n",
    "            dtype=dtypes,\n",
    "            date_cols=date_cols\n",
    "            # parse_dates=date_cols,\n",
    "        ),\n",
    "        dcd.read_parquet(\n",
    "            \"s3://otg-prod-fraud-data/test_data/nov24joined.parquet\",\n",
    "            sep=\",\",\n",
    "            dtype=dtypes,\n",
    "            date_cols=date_cols\n",
    "            # parse_dates=date_cols,\n",
    "        ),\n",
    "    ]\n",
    ").compute()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.datetime64('2024-10-01T00:00:00.000000'),\n",
       " numpy.datetime64('2024-11-30T00:00:00.000000'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(columns=[\"card_type\", \"payment_type\"], inplace=True)\n",
    "dataset.bus_date.min(),dataset.bus_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[dataset.bus_date.ge(\"2024-05-15\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 875 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "dataset[\"voucher_number\"] = dataset[\"voucher_number\"].apply(\n",
    "    lambda x: next((val for val in x if not pd.isna(val)), np.nan)\n",
    ")\n",
    "dataset[\"menu_vendor_id\"] = dataset[\"menu_vendor_id\"].apply(\n",
    "    lambda x: next((val for val in x if not pd.isna(val)), np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 s, sys: 2.2 s, total: 26.5 s\n",
      "Wall time: 31.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "INFERENCE_PIPELINE = pickle.load(\n",
    "    open(\n",
    "        \"/home/ec2-user/ai-studio-fraud-ml/aayush_exp/evaluation_notebook/New-Join/train_pipeline_new_join_data.pkl\",\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 9.67 s, total: 48.1 s\n",
      "Wall time: 48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset[\"FE_refund_without_reference\"] = (\n",
    "    (dataset[\"payment_amount\"] < 0) & (dataset[\"original_order_id\"].isna())\n",
    ").astype(int)\n",
    "\n",
    "dataset[\"FE_incomplete_refund\"] = (\n",
    "    (dataset[\"payment_amount\"] < 0) & (dataset[\"order_status_cd\"].isna())\n",
    ").astype(int)\n",
    "\n",
    "dataset[\"FE_original_missing\"] = (\n",
    "    ~dataset[\"original_order_id\"].isin(dataset[\"order_id\"])\n",
    "    & dataset[\"original_order_id\"].notna()\n",
    ")\n",
    "dataset[\"FE_original_missing\"] = dataset[\"FE_original_missing\"].astype(int)\n",
    "\n",
    "dataset[dataset.select_dtypes(\"object\").columns] = dataset.select_dtypes(\n",
    "    \"object\"\n",
    ").where(dataset.select_dtypes(\"object\").notna(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4873407, 73)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shap_pipeline import FraudDetectionModel\n",
    "\n",
    "final_pipeline = FraudDetectionModel(INFERENCE_PIPELINE,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.1 s, sys: 3.52 s, total: 12.6 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset[\"exclusive_tax\"] = None\n",
    "dataset[\"inclusive_tax\"] = None\n",
    "dataset['card_cash'] = None\n",
    "dataset['card_glory'] = None\n",
    "dataset['card_none'] = None\n",
    "dataset['payment_none'] = None\n",
    "dataset['card_other'] = None\n",
    "dataset['card_visa credit'] = None\n",
    "dataset['card_visa debit'] = None\n",
    "dataset['payment_gift'] = None\n",
    "dataset['card_paypal'] = None ### June\n",
    "dataset = dataset.sort_values(by = 'bus_date').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(enable_metadata_routing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2728742, 75)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape\n",
    "dataset[dataset.bus_date.dt.month==6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.index = dataset.index.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ComputeDateFeatures\n",
      "INFO:root:CustomerDaywiseSubtotal\n",
      "INFO:root:SubtotalOverFifty\n",
      "INFO:root:RefundOverFifteenDays\n",
      "INFO:root:RefundAgainstIncomplete\n",
      "INFO:root:TipGreaterThan50EmpMarker\n",
      "INFO:root:TipGreaterThan50Marker\n",
      "INFO:root:TipGreaterThan50VoucherMarker\n",
      "INFO:root:calculate_ratio_features\n",
      "INFO:root:CombineFeatures\n",
      "INFO:root:UniqueCounts\n",
      "INFO:root:Aggregation\n",
      "INFO:root:TimeblockFrequencyEncoder\n",
      "INFO:root:TimeblockFrequencyEncoder\n",
      "INFO:root:drop_columns\n",
      "INFO:root:FreqEncoder - Transform\n",
      "INFO:root:Prediction data shape:(2362908, 117)\n",
      "INFO:root:Filter columns over 0.05 pred prob\n",
      "INFO:root:SHAP explainer initialized\n",
      "INFO:root:SHAP feature generation\n",
      "/tmp/ipykernel_51609/848347536.py:58: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  top_features = df.columns[\n",
      "INFO:root:Sorting dataframes by probability\n",
      "INFO:root:Sorting dataframes by probability\n",
      "INFO:root:Computing Fraud Categories\n",
      "INFO:root:Generating refined SHAP explanations\n",
      "INFO:root:Finalizing SHAP explanations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min, sys: 57.1 s, total: 24min 57s\n",
      "Wall time: 23min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "full_data, predicted_data = final_pipeline.predict(\n",
    "    dataset,\n",
    "    dataset[dataset.bus_date.dt.month == 11].shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2987, 139) (2362908, 86)\n"
     ]
    }
   ],
   "source": [
    "print(predicted_data.shape, full_data.shape)\n",
    "# full_data.to_csv(\"s3://otg-prod-fraud-data/test_date/preds/june_preds_full.csv\",index=False)\n",
    "# predicted_data.to_csv(\"s3://otg-prod-fraud-data/test_date/preds/june_preds.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reduction Fraud                   2170\n",
       "Tips Fraud                         444\n",
       "Voucher Fraud                      201\n",
       "Uncategorized Fraud                145\n",
       "Refund Fraud                        24\n",
       "Refund Fraud / Reduction Fraud       2\n",
       "Tips Fraud / Reduction Fraud         1\n",
       "Name: fraud_category, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_data.fraud_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        42\n",
      "2     10747\n",
      "3      2415\n",
      "4      2521\n",
      "5      2684\n",
      "6      2798\n",
      "7      2107\n",
      "8      2586\n",
      "9      2352\n",
      "10        1\n",
      "12        7\n",
      "Name: bus_date, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30362/522914245.py:16: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  labels[\"bus_date\"] = pd.to_datetime(labels.bus_date)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Timestamp('2024-05-29 00:00:00'), Timestamp('2024-06-30 00:00:00'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\n",
    "    \"s3://otg-prod-fraud-data/prod_data/labels/feb24-sep24.csv\",\n",
    "    dtype={\n",
    "        \"Flo OrderID\": \"object\",\n",
    "        \"Appetize OrderId\":\"object\"\n",
    "    },\n",
    ").rename(\n",
    "    columns={\n",
    "        \"Order Dt\": \"bus_date\",\n",
    "        \"Flo OrderID\": \"device_order_id\",\n",
    "        \"Appetize OrderId\": \"order_id\",\n",
    "        \"Observation Remark\": \"remarks\",\n",
    "    }\n",
    ")\n",
    "# labels.order_id = labels.order_id.astype(\"int\")\n",
    "labels[\"bus_date\"] = pd.to_datetime(labels.bus_date)\n",
    "# labels = labels['device_order_id']\n",
    "labels = labels.drop_duplicates(subset=['device_order_id'])\n",
    "print(labels.bus_date.dt.month.value_counts().sort_index())\n",
    "june = labels[labels.Month==\"Jun-24\"].drop_duplicates('device_order_id')\n",
    "june.bus_date.min(), june.bus_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439 1090 380\n"
     ]
    }
   ],
   "source": [
    "FP = predicted_data.device_order_id.drop_duplicates()[\n",
    "    ~predicted_data.device_order_id.drop_duplicates().isin(june[\"device_order_id\"])\n",
    "].shape[0]\n",
    "TP = predicted_data.device_order_id.drop_duplicates()[\n",
    "    predicted_data.device_order_id.drop_duplicates().isin(june[\"device_order_id\"])\n",
    "].shape[0]\n",
    "FN = (~june['device_order_id'].isin(predicted_data.device_order_id.drop_duplicates())).sum()\n",
    "print(TP, FP, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 649 ms, sys: 0 ns, total: 649 ms\n",
      "Wall time: 648 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>FP</th>\n",
       "      <th>TP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1090</td>\n",
       "      <td>2439</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>920</td>\n",
       "      <td>2424</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.07</td>\n",
       "      <td>812</td>\n",
       "      <td>2414</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08</td>\n",
       "      <td>726</td>\n",
       "      <td>2407</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.09</td>\n",
       "      <td>684</td>\n",
       "      <td>2392</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.95</td>\n",
       "      <td>5</td>\n",
       "      <td>1635</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.96</td>\n",
       "      <td>3</td>\n",
       "      <td>1582</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>1523</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1368</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "      <td>941</td>\n",
       "      <td>1878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold    FP    TP    FN\n",
       "0        0.05  1090  2439   380\n",
       "1        0.06   920  2424   395\n",
       "2        0.07   812  2414   405\n",
       "3        0.08   726  2407   412\n",
       "4        0.09   684  2392   427\n",
       "..        ...   ...   ...   ...\n",
       "90       0.95     5  1635  1184\n",
       "91       0.96     3  1582  1237\n",
       "92       0.97     0  1523  1296\n",
       "93       0.98     0  1368  1451\n",
       "94       0.99     0   941  1878\n",
       "\n",
       "[95 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "thresholds = np.round(np.arange(0.05, 1, 0.01), 3)\n",
    "\n",
    "cm_data = []\n",
    "\n",
    "\n",
    "for t in thresholds:\n",
    "    data = full_data[full_data.prediction_probabilities>=t].drop_duplicates('device_order_id')\n",
    "    \n",
    "    FP = data.device_order_id[~data.device_order_id.isin(june[\"device_order_id\"])]\n",
    "    TP = data.device_order_id[data.device_order_id.isin(june[\"device_order_id\"])]\n",
    "    FN = june.device_order_id[~june.device_order_id.isin(data[\"device_order_id\"])]\n",
    "    cm_data.append({\n",
    "        'Threshold': t,\n",
    "        'FP': FP.shape[0],\n",
    "        'TP': TP.shape[0],\n",
    "        'FN': FN.shape[0],\n",
    "        # 'TP($)': data[data.device_order_id.isin(TP)].reduction_amt.abs().sum(),\n",
    "        # 'FP($)': data[data.device_order_id.isin(FP)].reduction_amt.abs().sum(),\n",
    "        # 'FN($)': full_data[full_data.prediction_probabilities<t][full_data[full_data.prediction_probabilities<t].device_order_id.isin(FN)].reduction_amt.abs().sum()\n",
    "    })\n",
    "    # print(t)\n",
    "\n",
    "cm_data = pd.DataFrame(cm_data)\n",
    "# cm_data.to_csv(\"s3://otg-prod-fraud-data/new_predictions/monthly_new_category/results/confusion_values/non_duplicated/updated/august24_threshold_results.csv\",index=False)\n",
    "# Display the result\n",
    "cm_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/otg-fraud/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shap\n",
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.base import BaseEstimator, OneToOneFeatureMixin, TransformerMixin\n",
    "\n",
    "\n",
    "class FraudDetectionModel(BaseEstimator):\n",
    "    def __init__(self, train_pipeline, THRESHOLD=0.05):\n",
    "        pd.options.mode.chained_assignment = None\n",
    "\n",
    "        self.pipeline = train_pipeline\n",
    "        self.preprocess_pipeline = self.pipeline[\"pipeline\"]\n",
    "        self.model = self.pipeline[\"xgbclassifier\"]\n",
    "        self.explainer = shap.Explainer(self.model)\n",
    "        self.THRESHOLD = THRESHOLD\n",
    "\n",
    "    def _filter_columns_over_threshold(self, X):\n",
    "        \"\"\"\n",
    "        Filter the dataframe based on a specified threshold and returns the slice with prediction probabilities greater than or equal to the specified threshold.\n",
    "\n",
    "        Parameters:\n",
    "        X (pd.DataFrame | cudf.DataFrame): Dataframe to filter.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame | cudf.DataFrame: A slice of the input DataFrame containing rows where prediction probabilities are greater than or equal to the specified threshold.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.log(\n",
    "            level=logging.INFO, msg=f\"Filter columns over {self.THRESHOLD} pred prob\"\n",
    "        )\n",
    "\n",
    "        return X[X[\"prediction_probabilities\"] >= self.THRESHOLD]\n",
    "\n",
    "    def _shap_feature_generation(self, df, shap_values):\n",
    "        \"\"\"\n",
    "        Determine SHAP features from given SHAP values.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): Dataframe from which SHAP values are to be determined.\n",
    "        shap_values (numpy.ndarray): Matrix of SHAP values.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Dataframe with a column 'SHAP_features' containing list of 3 most important SHAP features for every transaction.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.log(level=logging.INFO, msg=\"SHAP feature generation\")\n",
    "\n",
    "        top_positive_shap_values = np.sort(shap_values)[\n",
    "            :, -3:\n",
    "        ]  # Determine the top 3 SHAP values\n",
    "        top_features_indices = np.argsort(\n",
    "            shap_values\n",
    "        )[\n",
    "            :, -3:\n",
    "        ]  # Determine indices of the 3 most important SHAP features, and store them to top_feature_indices\n",
    "        top_features = df.columns[\n",
    "            top_features_indices\n",
    "        ]  # Determine top 3 highest importance columns using the 'top_features_indices' list.\n",
    "\n",
    "        important_features = [\n",
    "            \"P_Imp1\",\n",
    "            \"P_Imp2\",\n",
    "            \"P_Imp3\",\n",
    "        ]\n",
    "        # Columns containing names of the 3 important features\n",
    "\n",
    "        new_data = pd.DataFrame(top_features, columns=important_features)\n",
    "\n",
    "        df = pd.concat(\n",
    "            [df.reset_index(drop=True), new_data.reset_index(drop=True)], axis=1\n",
    "        )\n",
    "\n",
    "        df[\"SHAP_features\"] = df[important_features].apply(\n",
    "            lambda row: \" + \".join(sorted(row)), axis=1\n",
    "        )\n",
    "\n",
    "        df.drop(columns=important_features, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _sort_by_probability(self, df):\n",
    "        \"\"\"\n",
    "        Sort dataframe by prediction probability\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): Dataframe which needs to be sorted by prediction probability\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: sorted dataframe.\n",
    "        \"\"\"\n",
    "        logging.log(level=logging.INFO, msg=\"Sorting dataframes by probability\")\n",
    "        return df.sort_values(\"prediction_probabilities\", ascending=False).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "    def _shap_explanation(self, shap_data_detailed):\n",
    "        logging.log(level=logging.INFO, msg=\"Generating refined SHAP explanations\")\n",
    "        # If id features needs to be converted into integer\n",
    "        non_exp_columns = [\n",
    "            x\n",
    "            for x in shap_data_detailed.columns\n",
    "            if x.endswith(\"_ratio\") or x.endswith(\"_ct_AG\")\n",
    "        ]\n",
    "        list_to_integer = [\"day_of_month\", \"day_of_week\", \"voucher_count\"]\n",
    "\n",
    "        shap_data_detailed[list_to_integer] = (\n",
    "            shap_data_detailed[list_to_integer]\n",
    "            .apply(pd.to_numeric, errors=\"coerce\")\n",
    "            .astype(pd.Int64Dtype())\n",
    "        )\n",
    "\n",
    "        # Rounding off the amount related columns to 2 digit places\n",
    "        round_columns = [\n",
    "            \"cust_daywise_subtotal\",\n",
    "            \"gross_total\",\n",
    "            \"net_total\",\n",
    "            \"taxes\",\n",
    "            \"exclusive_tax\",\n",
    "            \"inclusive_tax\",\n",
    "            \"reduction_amt\",\n",
    "            \"fee_amt\",\n",
    "            \"tip_amount\",\n",
    "            \"payment_amount\",\n",
    "        ]\n",
    "\n",
    "        shap_data_detailed[round_columns] = shap_data_detailed[round_columns].round(2)\n",
    "\n",
    "        # Replacing flag features with True and False value\n",
    "        flag_features = [\n",
    "            \"FE_subtotal_over_50_fl\",\n",
    "            \"FE_refund_over_15_days_fl\",\n",
    "            \"FE_refund_against_incomplete_fl\",\n",
    "            \"FE_multiple_voucher_fl\",\n",
    "            \"FE_tip_gt_50_fl\",\n",
    "            \"FE_tip_gt_50_voucher_fl\",\n",
    "            \"FE_tip_gt_50_emp_fl\",\n",
    "        ]\n",
    "        for feature in flag_features:\n",
    "            shap_data_detailed[feature] = shap_data_detailed[feature].replace(\n",
    "                {1: True, -1: False, 0: False}\n",
    "            )\n",
    "\n",
    "        shap_data_detailed[\"shap_detailed_data\"] = shap_data_detailed[\n",
    "            \"SHAP_features\"\n",
    "        ].apply(lambda x: [i for i in x.split(\" + \")])\n",
    "\n",
    "        def shap_helper(x):\n",
    "            values = []\n",
    "            for features in x:\n",
    "                for y in shap_data_detailed.columns:\n",
    "                    if (\n",
    "                        features not in non_exp_columns\n",
    "                        and pd.notna(features)\n",
    "                        and re.search(re.escape(y), features)\n",
    "                    ):\n",
    "                        values.append(y)\n",
    "                    elif features in non_exp_columns:\n",
    "                        if features in values:\n",
    "                            continue\n",
    "                        values.append(features)\n",
    "                    else:\n",
    "                        continue\n",
    "            return values\n",
    "\n",
    "        shap_data_detailed[\"shap_detailed_data\"] = shap_data_detailed[\n",
    "            \"shap_detailed_data\"\n",
    "        ].apply(shap_helper)\n",
    "\n",
    "        shap_data_detailed[\"shap_detailed_data\"] = shap_data_detailed[\n",
    "            \"shap_detailed_data\"\n",
    "        ].apply(\n",
    "            lambda features: [\n",
    "                item\n",
    "                for item in features\n",
    "                if not item.endswith((\"CB\", \"FE\", \"id_AG\", \"FE_subtotal_over_50_fl\"))\n",
    "                # if not item.endswith((\"CB\", \"FE\", \"AG\", \"ratio\", \"order_id\"))\n",
    "            ]\n",
    "            if features\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        # Adding cust_prof_id in the beginning if it is present in the list else not adding.\n",
    "        shap_data_detailed[\"shap_detailed_data_filtered\"] = shap_data_detailed[\n",
    "            \"shap_detailed_data\"\n",
    "        ].apply(\n",
    "            lambda features: (\n",
    "                \", \".join(\n",
    "                    [\"cust_prof_id\"]\n",
    "                    + sorted(set(item.strip() for item in features) - {\"cust_prof_id\"})\n",
    "                )\n",
    "                if features and \"cust_prof_id\" in features\n",
    "                else \", \".join(set(item.strip() for item in features))\n",
    "                if features\n",
    "                else np.nan\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Adding gross_total and net_total if reduction_amt is present\n",
    "        shap_data_detailed[\"shap_detailed_data_filtered\"] = shap_data_detailed[\n",
    "            \"shap_detailed_data_filtered\"\n",
    "        ].apply(\n",
    "            lambda features: features + \", gross_total, net_total\"\n",
    "            if pd.notna(features)\n",
    "            and \"reduction_amt\" in features\n",
    "            and \"gross_total\" not in features\n",
    "            and \"net_total\" not in features\n",
    "            else features\n",
    "        )\n",
    "\n",
    "        # Adding prediction probabilities in front of every explanation.\n",
    "        shap_data_detailed[\"shap_detailed_data_filtered\"] = shap_data_detailed[\n",
    "            \"shap_detailed_data_filtered\"\n",
    "        ].apply(\n",
    "            lambda features: \"prediction_probabilities, \" + features\n",
    "            if pd.notna(features)\n",
    "            else features\n",
    "        )\n",
    "\n",
    "        # Feature mapping has been done manually for all feature engineered columns.\n",
    "        feature_mapping = {\n",
    "            \"prediction_probabilities\": \"This transaction is predicted\",\n",
    "            \"cust_prof_id\": \"for Customer ID\",\n",
    "            \"sales_hr\": \"Transaction done on hour\",\n",
    "            \"cust_first_name\": \"Customer First Name\",\n",
    "            \"cust_last_name\": \"Customer last name\",\n",
    "            \"day_of_month\": \"Day of month\",\n",
    "            \"day_of_week\": \"Day of week\",\n",
    "            \"reduction_cd\": \"Reduction Code used by employee\",\n",
    "            \"vendor_loc_id\": \"Vendor location ID\",\n",
    "            \"vendor_id\": \"Vendor ID\",\n",
    "            \"pos_venue_id\": \"Pos Venue ID\",\n",
    "            \"payment_type\": \"Payment type used\",\n",
    "            \"order_status_cd\": \"Order Status Code\",\n",
    "            \"menu_vendor_id\": \"Menu Vendor ID\",\n",
    "            \"cust_daywise_subtotal\": \"Customer Daywise Subtotal\",\n",
    "            \"voucher_count\": \"\"\"Voucher is used {count} times\"\"\",\n",
    "            \"Refund_without_Reference\": \"Refund is claimed but no original order id value mentioned\",\n",
    "            \"Incomplete_Refund\": \"Refund is claimed and order status cd\",\n",
    "            \"Original_id_missing\": \"Order ID corresponding to original order id within 1 month not found\",\n",
    "            \"taxes\": \"Taxes paid\",\n",
    "            \"payment_amount\": \"Payment amount for the transaction\",\n",
    "            \"reduction_amt\": \"reduction_amt\",\n",
    "            \"net_total\": \"Net Total\",\n",
    "            \"gross_total\": \"Gross Total for the transaction\",\n",
    "            \"tip_amount\": \"Tip amount received for the transacation\",\n",
    "            \"bus_date\": \"Business Transaction date\",\n",
    "            \"device_order_id\": \"Device order ID\",\n",
    "            # \"voucher_number\": \"Voucher Number Used for payment\",\n",
    "            \"voucher_number\": \"\",\n",
    "            \"order_id\": \"Order ID\",\n",
    "            \"activity_status_code\": \"Activity status code\",\n",
    "            \"card_type\": \"Card Type used for payment\",\n",
    "            \"pos_terminal_id\": \"Pos Terminal ID\",\n",
    "        }\n",
    "\n",
    "        combined_feature_mapping = {\n",
    "            # Flag Features\n",
    "            \"FE_subtotal_over_50_fl\": \"Subtotal over $50 given\",\n",
    "            \"FE_refund_over_15_days_fl\": \"Refund Over 15 days\",\n",
    "            \"FE_refund_against_incomplete_fl\": \"Refund given to original order id having null status\",\n",
    "            \"FE_multiple_voucher_fl\": \"Multiple voucher used\",\n",
    "            \"FE_tip_gt_50_fl\": \"Tip greater than 50% of gross total\",\n",
    "            \"FE_tip_gt_50_emp_fl\": \"Tip greater than 50% of gross total and employee id given\",\n",
    "            \"FE_tip_gt_50_voucher_fl\": \"Tip greater than 50% of gross total and voucher used\",\n",
    "            # Ratio Features\n",
    "            \"FE_taxes_by_payment_amount_ratio\": \"\"\"Tax amount is {ratio}% of Payment Amount\"\"\",\n",
    "            \"FE_reduction_amt_by_payment_amount_ratio\": \"\"\"Reduction Amount is {ratio}% of Payment Amount\"\"\",\n",
    "            \"FE_net_total_by_gross_total_ratio\": \"\"\"Net Total Amount is {ratio}% of Gross Total\"\"\",\n",
    "            \"FE_tip_amount_by_payment_amount_ratio\": \"\"\"Tip amount is {ratio}% of Payment Amount\"\"\",\n",
    "            \"FE_tip_amount_by_gross_total_ratio\": \"\"\"Tip amount is {ratio}% of Gross Total\"\"\",\n",
    "            # Count Agregation Features\n",
    "            \"FE_device_order_id_voucher_number_ct_AG\": \"\"\"this device order id has {count} different voucher numbers\"\"\",\n",
    "            \"FE_cust_prof_id_voucher_number_ct_AG\": \"\"\"this customer prof id has {count} different voucher numbers\"\"\",\n",
    "            \"FE_device_order_id_order_id_ct_AG\": \"\"\"this device order id has {count} different order ids\"\"\",\n",
    "            \"FE_cust_prof_id_order_id_ct_AG\": \"\"\"this customer prof id has {count} different order ids\"\"\",\n",
    "        }\n",
    "\n",
    "        dollar_features = [\n",
    "            \"gross_total\",\n",
    "            \"net_total\",\n",
    "            \"taxes\",\n",
    "            \"exclusive_tax\",\n",
    "            \"inclusive_tax\",\n",
    "            \"fee_amt\",\n",
    "            \"tip_amount\",\n",
    "            \"payment_amount\",\n",
    "            \"cust_daywise_subtotal\",\n",
    "        ]\n",
    "\n",
    "        shap_data_detailed[\"Detailed_features_with_values\"] = shap_data_detailed.apply(\n",
    "            lambda row: \",  \".join(\n",
    "                [\n",
    "                    f\"{feature_mapping.get(feature, feature)} is ${row[feature]}\"\n",
    "                    if feature in dollar_features\n",
    "                    else f\"{feature_mapping.get(feature, feature)} {row['fraud_category']} with probability {int(row[feature] * 100)}%\"\n",
    "                    if feature == \"prediction_probabilities\"\n",
    "                    and \"cust_prof_id\" in row[\"shap_detailed_data_filtered\"].split(\", \")\n",
    "                    else f\"{feature_mapping.get(feature, feature)} {row['fraud_category']} with probability {int(row[feature]*100)}%  because\"\n",
    "                    if feature == \"prediction_probabilities\"\n",
    "                    and \"cust_prof_id\"\n",
    "                    not in row[\"shap_detailed_data_filtered\"].split(\", \")\n",
    "                    else f\"{feature_mapping.get(feature, feature)} {row[feature]} because\"\n",
    "                    if feature == \"cust_prof_id\"\n",
    "                    else f\"{feature_mapping.get(feature,feature)} is ${abs(row[feature])} which is {(abs(row['reduction_amt'])/row['gross_total'])*100:.2f}% of gross total\"\n",
    "                    if feature == \"reduction_amt\" and (row[\"gross_total\"] != 0)\n",
    "                    else f\"{feature_mapping.get(feature,feature)} is {row[feature]}, reduction amount is ${abs(row['reduction_amt'])} which is {(abs(row['reduction_amt'])/row['gross_total'])*100:.2f}% of gross total\"\n",
    "                    if feature == \"reduction_cd\" and (row[\"gross_total\"] != 0)\n",
    "                    else f\"{feature_mapping.get(feature,feature)} is {row['order_status_cd']}\"\n",
    "                    if feature == \"Incomplete_Refund\"\n",
    "                    # Handling Voucher number and voucher count feature\n",
    "                    else f\"Voucher number is {row['voucher_number']}\"\n",
    "                    if (feature == \"voucher_count\") and (row[\"voucher_count\"] >= 20)\n",
    "                    else f\"{feature_mapping.get(feature, feature).format(count=row['voucher_count'])} and voucher number is {row['voucher_number']}\"\n",
    "                    if (feature == \"voucher_count\") and (row[\"voucher_count\"] < 20)\n",
    "                    # Handling Voucher number duplication\n",
    "                    else \"\"\n",
    "                    if feature == \"voucher_number\"\n",
    "                    else f\"{combined_feature_mapping.get(feature,feature)}\"\n",
    "                    if feature == \"FE_refund_against_incomplete_fl\"\n",
    "                    # Handling ratio features\n",
    "                    else f\"{combined_feature_mapping.get(feature,feature)} is {row[feature]}\"\n",
    "                    if feature.endswith(\"_fl\")\n",
    "                    # Handling ratio features\n",
    "                    else f\"{combined_feature_mapping.get(feature,feature).format(ratio=f'{row[feature]* 100:.2f}')}\"\n",
    "                    if feature.endswith(\"ratio\")\n",
    "                    # Handling Count Aggregate features\n",
    "                    else f\"{combined_feature_mapping.get(feature,feature).format(count=row[feature])}\"\n",
    "                    if (feature.endswith(\"_ct_AG\")) & (row[feature] != 1)\n",
    "                    else \"\"\n",
    "                    if feature.endswith(\"_ct_AG\")\n",
    "                    # Remaining Missed Feature Mapping\n",
    "                    else f\"{feature_mapping.get(feature, feature)} is {row[feature]}\"\n",
    "                    for feature in row[\"shap_detailed_data_filtered\"].split(\", \")\n",
    "                ]\n",
    "            )\n",
    "            if pd.notna(row[\"shap_detailed_data_filtered\"])\n",
    "            else np.nan,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        shap_data_detailed[\"shap_detailed_data_filtered\"] = shap_data_detailed[\n",
    "            \"shap_detailed_data_filtered\"\n",
    "        ].apply(lambda features: features if pd.notna(features) else np.nan)\n",
    "\n",
    "        return shap_data_detailed\n",
    "\n",
    "    def _classify_fraud_category(self, row):\n",
    "        features = row[\"SHAP_features\"].split(\" + \")\n",
    "        tip_amount = row[\"tip_amount\"] if pd.notna(row[\"tip_amount\"]) else 0\n",
    "        # tip_amount = row[\"tip_amount\"]\n",
    "        fraud_categories = []\n",
    "\n",
    "        refund_features = [\n",
    "            \"Original_id_missing\",\n",
    "            \"Incomplete_Refund\",\n",
    "            \"FE_refund_against_incomplete_fl\",\n",
    "        ]\n",
    "        refund_count = sum(feature in features for feature in refund_features)\n",
    "\n",
    "        tips_feature = [\n",
    "            \"FE_tip_gt_50_emp_fl\",\n",
    "            \"FE_tip_amount_by_payment_amount_ratio\",\n",
    "            \"FE_tip_amount_by_gross_total_ratio\",\n",
    "        ]\n",
    "        tips_count = sum(feature in features for feature in tips_feature)\n",
    "\n",
    "        if refund_count >= 2:\n",
    "            return \"Refund Fraud\"\n",
    "        elif refund_count == 1 and \"FE_device_order_id_order_id_ct_AG\" in features:\n",
    "            return \"Refund Fraud\"\n",
    "        elif refund_count == 1:\n",
    "            fraud_categories.append(\"Refund Fraud\")\n",
    "\n",
    "        if tips_count >= 2:\n",
    "            return \"Tips Fraud\"\n",
    "        elif tips_count == 1 and int(tip_amount) > 0:\n",
    "            fraud_categories.append(\"Tips Fraud\")\n",
    "\n",
    "        if sum(\"reduction\" in feature.lower() for feature in features) >= 2:\n",
    "            return \"Reduction Fraud\"\n",
    "\n",
    "        discount_conditions = [\n",
    "            \"reduction_cd\",\n",
    "            \"FE_subtotal_over_50_fl\",\n",
    "            \"mean_reduction_amt\",\n",
    "            \"FE_employee_role_name_reduction_cd_CB\",\n",
    "            \"std_reduction_amt\",\n",
    "            \"mean_discount_per_cust\",\n",
    "            \"cust_daywise_subtotal\",\n",
    "            \"gross_total\",\n",
    "            \"net_total\",\n",
    "        ]\n",
    "\n",
    "        if any(feature in features for feature in discount_conditions) and any(\n",
    "            \"reduction\" in feature.lower() for feature in features\n",
    "        ):\n",
    "            fraud_categories.append(\"Reduction Fraud\")\n",
    "\n",
    "        if sum(\"voucher\" in feature.lower() for feature in features) >= 2:\n",
    "            return \"Voucher Fraud\"\n",
    "\n",
    "        # Voucher Fraud\n",
    "        if (\"voucher_count\" in features or \"voucher_number\" in features) and any(\n",
    "            feature in features\n",
    "            for feature in [\"taxes\", \"pos_terminal_id\", \"gross_total\", \"net_total\"]\n",
    "        ):\n",
    "            fraud_categories.append(\"Voucher Fraud\")\n",
    "\n",
    "        elif \"FE_device_order_id_order_id_ct_AG\" in features and any(\n",
    "            feature in features\n",
    "            for feature in [\n",
    "                \"taxes\",\n",
    "                \"voucher_count\",\n",
    "                \"voucher_number\",\n",
    "                \"net_total\",\n",
    "                \"gross_total\",\n",
    "            ]\n",
    "        ):\n",
    "            fraud_categories.append(\"Voucher Fraud\")\n",
    "\n",
    "        if fraud_categories:\n",
    "            return \" / \".join(fraud_categories)\n",
    "        else:\n",
    "            return \"Uncategorized Fraud\"\n",
    "\n",
    "    def predict(self, raw_test_data, raw_minute_shape):\n",
    "        raw_test_data = raw_test_data.sort_values(by=\"bus_date\")\n",
    "        raw_minute_data = raw_test_data.iloc[-raw_minute_shape:]\n",
    "        # raw_minute_data = raw_test_data.iloc[-raw_minute_shape :]\n",
    "\n",
    "        preprocessed_test_data = self.preprocess_pipeline.transform(raw_test_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for col in [\"FE_multiple_voucher_fl\",\"fee_amt\", \"item_count\", \"mobile_ord_fl\", \"payment_amt_rewards_points\", \"reduction_amt\", \"taxes\", \"tip_amount\", \"FE_taxes_by_payment_amount_ratio\", \"FE_reduction_amt_by_payment_amount_ratio\", \"FE_tip_amount_by_payment_amount_ratio\", \"FE_tip_amount_by_gross_total_ratio\"]:\n",
    "            preprocessed_test_data[col]=pd.to_numeric(preprocessed_test_data[col])\n",
    "        \n",
    "        preprocessed_test_data = preprocessed_test_data.iloc[-raw_minute_shape:]\n",
    "\n",
    "        logging.log(\n",
    "            level=logging.INFO,\n",
    "            msg=f\"Prediction data shape:{preprocessed_test_data.shape}\",\n",
    "        )\n",
    "\n",
    "        predictions = self.model.predict_proba(\n",
    "            preprocessed_test_data\n",
    "        )  # numpy array of predictions\n",
    "\n",
    "        # Add column of prediction probabilities to both raw and preprocessed data\n",
    "        raw_minute_data[\"prediction_probabilities\"] = predictions[:, 1]\n",
    "        preprocessed_test_data[\"prediction_probabilities\"] = predictions[:, 1]\n",
    "\n",
    "        # Filter all rows over 0.1 threshold\n",
    "        _data_for_shap = self._filter_columns_over_threshold(preprocessed_test_data)\n",
    "\n",
    "        # Generate SHAP feature importance values for the data\n",
    "        shap_values = self.explainer(\n",
    "            _data_for_shap.drop(columns=[\"prediction_probabilities\"]),\n",
    "            check_additivity=False,\n",
    "        )\n",
    "        logging.log(level=logging.INFO, msg=\"SHAP explainer initialized\")\n",
    "\n",
    "        # Generate SHAP features\n",
    "        SHAP_data = self._shap_feature_generation(_data_for_shap, shap_values.values)\n",
    "\n",
    "        # Sort both raw and SHAP data by probability\n",
    "        sorted_raw_data = self._sort_by_probability(raw_minute_data)\n",
    "        sorted_SHAP_data = self._sort_by_probability(SHAP_data)\n",
    "\n",
    "        # Generate list of additional columns, i.e. columns in sorted_raw_data not present in sorted_SHAP_data\n",
    "        additional_columns = [\n",
    "            col\n",
    "            for col in sorted_SHAP_data.columns\n",
    "            if col not in sorted_raw_data.columns\n",
    "        ]\n",
    "\n",
    "        # Join df of SHAP data with raw data\n",
    "        result_df = sorted_raw_data.join(sorted_SHAP_data[additional_columns])\n",
    "\n",
    "        shap_data_detailed = result_df[\n",
    "            result_df[\"prediction_probabilities\"] >= self.THRESHOLD\n",
    "        ]\n",
    "\n",
    "        logging.log(level=logging.INFO, msg=\"Computing Fraud Categories\")\n",
    "        shap_data_detailed[\"fraud_category\"] = shap_data_detailed.apply(\n",
    "            self._classify_fraud_category, axis=1\n",
    "        )\n",
    "\n",
    "        shap_data_refined = self._shap_explanation(shap_data_detailed)\n",
    "\n",
    "        logging.log(level=logging.INFO, msg=\"Finalizing SHAP explanations\")\n",
    "\n",
    "        # shap_data_refined[\"fraud_category\"] = shap_data_refined[\n",
    "        #     \"shap_detailed_data\"\n",
    "        # ].apply(self._classify_fraud_category)\n",
    "        # shap_data_refined[\"fraud_category\"] = shap_data_refined.apply(self._classify_fraud_category,axis=1)\n",
    "        # shap_data_refined[\"fraud_category\"] = shap_data_refined[\"fraud_category\"].apply(\n",
    "        #     lambda x: \", \".join(x)\n",
    "        # )\n",
    "\n",
    "        shap_data_refined.drop(\n",
    "            columns=[\"shap_detailed_data_filtered\", \"shap_detailed_data\"], inplace=True\n",
    "        )\n",
    "        result_df = result_df.drop(columns=additional_columns).join(\n",
    "            shap_data_refined[[\"Detailed_features_with_values\", \"fraud_category\"]],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        result_df[\"Detailed_features_with_values\"] = result_df[\n",
    "            \"Detailed_features_with_values\"\n",
    "        ].replace(\",  ,  \", \",\")\n",
    "\n",
    "        return result_df, shap_data_refined\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otg-fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
